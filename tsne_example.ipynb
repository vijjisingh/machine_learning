{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding as LLE\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import pandas as pd\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "df=pd.read_csv(\"Analysis.csv\")\n",
    "#titles = [line.rstrip() for line in open('allbooks.txt')]\n",
    "#print(titles)\n",
    "# copy tokenizer from sentiment example\n",
    "#stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HSDPA', 'frame', 'loss', 'over', 'ATM', 'following', 'eDCPS', 'feature', 'activation', 'Thank', 'you', 'for', 'contacting', 'the', 'samsung', 'Global', 'Welcome', 'Center', '.', 'Your', 'request', 'has', 'been', 'processed', 'and', 'ticket', 'number', 'your', 'reference', 'is', 'AR', '1-6264897', 'Please', 'that', 'when', 'follow-up', 'A', 'representative', 'will', 'contact', 'regarding', 'request.Regarding', 'this', 'issue', ',', 'we', 'observed', 'frame-loss', 'was', 'related', 'to', 'only', '5', 'sites', 'correlated', 'with', 'increasing', 'in', 'DL', 'traffic', 'To', 'confirm', 'correlation', 'share', 'applied', '?', 'When', 'are', '0603', '0480', '3715,3776', '3554', 'migrated', 'RNC109', 'Regarding', 'Kindly', 'find', 'investigation', 'of', 'After', 'checking', 'case', 'by', 'all', 'them', 'IMA', 'load', 'as', 'suffered', 'from', 'very', 'heavy', 'busy', 'hours', 'but', 'before', 'average', 'less', 'than', '%', '100', 'after', 'reached', 'Also', 'RLReconfigration', 'Prepare', 'Unsuccess_', 'RrmRefusal', 'For', 'site', '01_S_SG-GIRGA-ENT_0603UP', 'which', 'resolved', 'faulty', 'PCM', 'recovered', 'means', 'problem', 'raised', 'due', 'lack', 'BW', 'there', 'no', 'had', 'From', 'above', 'outcome', 'I', 'think', 'available', 'should', 'be', 'upgrade', 'or', 'at', 'least', 'capacity', 'assessment', 'on', 'those', 'requested', 'resources', 'engineering', 'team', 'errors', 'AAL2', 'NB', 'CPs', 'lenth', 'mismatch', 'lost', 'CPS', 'About', 'between', 'can', 'check', 'rollback', 'availability', 'proceed', 'escalation', 'output.Kindly', 'output', 'start', 'initial', 'analysis', 'need', 'clarifications/information', '1', ')', 'provide', 'logs', 'location', 'collected', '2', 'Have', 'data', 'enabling', '3', 'please', 'Load', 'IMA/ATM', 'affected', 'Nodeb', \"'s\", 'migration', '4', 'How', 'many', 'nodebs', 'part', 'were', 'nodes', 'got', 'As', 'per', 'description', 'replacement', 'Problem', 'So', 'new', 'wire', 'Source', '6', 'Form', 'notes', 'it', 'found', 'issues', '``', \"''\", 'details', 'error', 'how', 'these', 'identified', '7', 'collect', 'HFB', 'day', 'till', 'date', 'transport', 'side', '8', 'IMT', 'nodeb', 'internal', 'discussion', 'have', 'any', 'effects', 'IUB', 'bandwidth', 'TBM', 'control', 'CAC', 'still', 'understand', 'form', 'POV']\n",
      "100 241\n",
      "vocab size: 241\n",
      "24.1 2 241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\py35\\lib\\site-packages\\ipykernel_launcher.py:101: RuntimeWarning: invalid value encountered in true_divide\n",
      "c:\\py35\\lib\\site-packages\\matplotlib\\colors.py:233: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any((result < 0) | (result > 1)):\n",
      "c:\\py35\\lib\\site-packages\\matplotlib\\colors.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  if np.any((result < 0) | (result > 1)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0 -> ['HSDPA', 'frame', 'loss', 'over', 'ATM', 'following', 'eDCPS', 'feature', 'activation', 'Thank', 'you', 'for', 'contacting', 'the', 'samsung', 'Global', 'Welcome', 'Center', '.', 'Your', 'request', 'has', 'been', 'processed', 'and', 'ticket', 'number', 'your', 'reference', 'is', 'AR', '1-6264897', 'Please', 'that', 'when', 'follow-up', 'A', 'representative', 'will', 'contact', 'regarding', 'request.Regarding', 'this', 'issue', ',', 'we', 'observed', 'frame-loss', 'was', 'related', 'to', 'only', '5', 'sites', 'correlated', 'with', 'increasing', 'in', 'DL', 'traffic', 'To', 'confirm', 'correlation', 'share', 'applied', '?', 'When', 'are', '0603', '0480', '3715,3776', '3554', 'migrated', 'RNC109', 'Regarding', 'Kindly', 'find', 'investigation', 'of', 'After', 'checking', 'case', 'by', 'all', 'them', 'IMA', 'load', 'as', 'suffered', 'from', 'very', 'heavy', 'busy', 'hours', 'but', 'before', 'average', 'less', 'than', '%', '100', 'after', 'reached', 'Also', 'RLReconfigration', 'Prepare', 'Unsuccess_', 'RrmRefusal', 'For', 'site', '01_S_SG-GIRGA-ENT_0603UP', 'which', 'resolved', 'faulty', 'PCM', 'recovered', 'means', 'problem', 'raised', 'due', 'lack', 'BW', 'there', 'no', 'had', 'From', 'above', 'outcome', 'I', 'think', 'available', 'should', 'be', 'upgrade', 'or', 'at', 'least', 'capacity', 'assessment', 'on', 'those', 'requested', 'resources', 'engineering', 'team', 'errors', 'AAL2', 'NB', 'CPs', 'lenth', 'mismatch', 'lost', 'CPS', 'About', 'between', 'can', 'check', 'rollback', 'availability', 'proceed', 'escalation', 'output.Kindly', 'output', 'start', 'initial', 'analysis', 'need', 'clarifications/information', '1', ')', 'provide', 'logs', 'location', 'collected', '2', 'Have', 'data', 'enabling', '3', 'please', 'Load', 'IMA/ATM', 'affected', 'Nodeb', \"'s\", 'migration', '4', 'How', 'many', 'nodebs', 'part', 'were', 'nodes', 'got', 'As', 'per', 'description', 'replacement', 'Problem', 'So', 'new', 'wire', 'Source', '6', 'Form', 'notes', 'it', 'found', 'issues', '``', \"''\", 'details', 'error', 'how', 'these', 'identified', '7', 'collect', 'HFB', 'day', 'till', 'date', 'transport', 'side', '8', 'IMT', 'nodeb', 'internal', 'discussion', 'have', 'any', 'effects', 'IUB', 'bandwidth', 'TBM', 'control', 'CAC', 'still', 'understand', 'form', 'POV']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan],\n",
       "        [ nan,  nan]]), array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "        ..., \n",
       "        [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan, ...,  nan,  nan,  nan]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [line.rstrip() for line in open('issue.txt')]\n",
    "#print(titles)\n",
    "# copy tokenizer from sentiment example\n",
    "#stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "#print(stopwords)\n",
    "# add more stopwords specific to this problem\n",
    "stopwords = stopwords.union({\n",
    "    'introduction', 'edition', 'series', 'application',\n",
    "    'approach', 'card', 'access', 'package', 'plus', 'etext',\n",
    "    'brief', 'vol', 'fundamental', 'guide', 'essential', 'printed',\n",
    "    'third', 'second', 'fourth', })\n",
    "def my_tokenizer(s):\n",
    "    #s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    #tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] # remove any digits, i.e. \"3rd edition\"\n",
    "    print(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "all_tokens = []\n",
    "all_titles = []\n",
    "index_word_map = []\n",
    "for title in titles:\n",
    "    #title = title.encode('ascii', 'ignore') # this will throw exception if bad characters\n",
    "    all_titles.append(title)\n",
    "    tokens = nltk.tokenize.word_tokenize(title)\n",
    "    all_tokens.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            index_word_map.append(token)\n",
    "\n",
    "#print(all_tokens)\n",
    "#print(all_titles)\n",
    "print(index_word_map)\n",
    "\n",
    "# now let's create our input matrices - just indicator variables for this example - works better than proportions\n",
    "def tokens_to_vector(tokens):\n",
    "    x = np.zeros(len(word_index_map))\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    return x\n",
    "\n",
    "N = len(all_tokens)\n",
    "D = len(word_index_map)\n",
    "print(N,D)\n",
    "X = np.zeros((D, N))# terms will go along rows, documents along columns\n",
    "#print(X)\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    X[:,i] = tokens_to_vector(tokens)\n",
    "    i += 1\n",
    "\n",
    "def d(u, v):\n",
    "    diff = u - v\n",
    "    return diff.dot(diff)\n",
    "\n",
    "def cost(X, R, M):\n",
    "    cost = 0\n",
    "    for k in range(len(M)):\n",
    "        # method 1\n",
    "        # for n in xrange(len(X)):\n",
    "        #     cost += R[n,k]*d(M[k], X[n])\n",
    "\n",
    "        # method 2\n",
    "        diff = X - M[k]\n",
    "        sq_distances = (diff * diff).sum(axis=1)\n",
    "        cost += (R[:,k] * sq_distances).sum()\n",
    "    return cost\n",
    "\n",
    "def plot_k_means(X, K, index_word_map, max_iter=20, beta=1.0, show_plots=True):\n",
    "    N, D = X.shape\n",
    "    print(K,D,N)\n",
    "    K=int(K)\n",
    "    M = np.zeros((K, D))\n",
    "    R = np.zeros((N, K))\n",
    "    exponents = np.empty((N, K))\n",
    "\n",
    "    # initialize M to random\n",
    "    for k in range(K):\n",
    "        M[k] = X[np.random.choice(N)]\n",
    "\n",
    "    costs = np.zeros(max_iter)\n",
    "    for i in range(max_iter):\n",
    "        # step 1: determine assignments / resposibilities\n",
    "        # is this inefficient?\n",
    "        for k in range(K):\n",
    "            for n in range(N):\n",
    "                # R[n,k] = np.exp(-beta*d(M[k], X[n])) / np.sum( np.exp(-beta*d(M[j], X[n])) for j in xrange(K) )\n",
    "                exponents[n,k] = np.exp(-beta*d(M[k], X[n]))\n",
    "\n",
    "        R = exponents / exponents.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # step 2: recalculate means\n",
    "        for k in range(K):\n",
    "            M[k] = R[:,k].dot(X) / R[:,k].sum()\n",
    "\n",
    "        costs[i] = cost(X, R, M)\n",
    "        if i > 0:\n",
    "            if np.abs(costs[i] - costs[i-1]) < 10e-5:\n",
    "                break\n",
    "\n",
    "    if show_plots:\n",
    "        # plt.plot(costs)\n",
    "        # plt.title(\"Costs\")\n",
    "        # plt.show()\n",
    "\n",
    "        random_colors = np.random.random((K, 3))\n",
    "        colors = R.dot(random_colors)\n",
    "        plt.figure(figsize=(80.0, 80.0))\n",
    "        plt.scatter(X[:,0], X[:,1], s=300, alpha=0.9, c=colors)\n",
    "        annotate1(X, index_word_map)\n",
    "        # plt.show()\n",
    "        plt.savefig(\"test2.png\")\n",
    "\n",
    "\n",
    "    # print out the clusters\n",
    "    hard_responsibilities = np.argmax(R, axis=1) # is an N-size array of cluster identities\n",
    "    # let's \"reverse\" the order so it's cluster identity -> word index\n",
    "    cluster2word = {}\n",
    "    for i in range(len(hard_responsibilities)):\n",
    "      word = index_word_map[i]\n",
    "      cluster = hard_responsibilities[i]\n",
    "      if cluster not in cluster2word:\n",
    "        cluster2word[cluster] = []\n",
    "      cluster2word[cluster].append(word)\n",
    "\n",
    "    # print out the words grouped by cluster\n",
    "    for cluster, wordlist in cluster2word.items():\n",
    "      print(\"cluster\", cluster, \"->\", wordlist)\n",
    "\n",
    "    return M, R\n",
    "\n",
    "# def annotate2(X, index_word_map, k=0.1):\n",
    "#   N, D = X.shape\n",
    "\n",
    "#   # create graph\n",
    "#   G = nx.DiGraph()\n",
    "#   data_nodes = []\n",
    "#   init_pos = {}\n",
    "#   for i in xrange(N):\n",
    "#     x, y = X[i]\n",
    "#     label = index_word_map[i]\n",
    "#     data_str = 'data_{0}'.format(label)\n",
    "#     G.add_node(data_str)\n",
    "#     G.add_node(label)\n",
    "#     G.add_edge(label, data_str)\n",
    "#     data_nodes.append(data_str)\n",
    "#     init_pos[data_str] = (x, y)\n",
    "#     init_pos[label] = (x, y)\n",
    "#   pos = nx.spring_layout(G, pos=init_pos, fixed=data_nodes, k=k)\n",
    "\n",
    "#   # undo spring_layout's rescaling\n",
    "#   pos_after = np.vstack([pos[d] for d in data_nodes])\n",
    "#   pos_before = np.vstack([init_pos[d] for d in data_nodes])\n",
    "#   scale, shift_x = np.polyfit(pos_after[:,0], pos_before[:,0], 1)\n",
    "#   scale, shift_y = np.polyfit(pos_after[:,1], pos_before[:,1], 1)\n",
    "#   shift = np.array([shift_x, shift_y])\n",
    "#   for key, val in pos.items():\n",
    "#     pos[key] = (val*scale) + shift\n",
    "\n",
    "\n",
    "#   for label, data_str in G.edges():\n",
    "#     plt.annotate(\n",
    "#       label,\n",
    "#       xy=pos[data_str], xycoords='data',\n",
    "#       xytext=pos[label], textcoords='data',\n",
    "#       arrowprops=dict(arrowstyle=\"->\", color='black'),\n",
    "#     )\n",
    "\n",
    "#   # expand limits\n",
    "#   all_pos = np.vstack(pos.values())\n",
    "#   x_span, y_span = np.ptp(all_pos, axis=0)\n",
    "#   mins = np.min(all_pos-x_span*0.15, 0)\n",
    "#   maxs = np.max(all_pos+y_span*0.15, 0)\n",
    "#   plt.xlim([mins[0], maxs[0]])\n",
    "#   plt.ylim([mins[1], maxs[1]])\n",
    "\n",
    "\n",
    "def annotate1(X, index_word_map, eps=0.1):\n",
    "  N, D = X.shape\n",
    "  placed = np.empty((N, D))\n",
    "  for i in range(N):\n",
    "    x, y = X[i]\n",
    "\n",
    "    # if x, y is too close to something already plotted, move it\n",
    "    close = []\n",
    "\n",
    "    x, y = X[i]\n",
    "    for retry in range(3):\n",
    "      for j in range(i):\n",
    "        diff = np.array([x, y]) - placed[j]\n",
    "\n",
    "        # if something is close, append it to the close list\n",
    "        if diff.dot(diff) < eps:\n",
    "          close.append(placed[j])\n",
    "\n",
    "      if close:\n",
    "        # then the close list is not empty\n",
    "        x += (np.random.randn() + 0.5) * (1 if np.random.rand() < 0.5 else -1)\n",
    "        y += (np.random.randn() + 0.5) * (1 if np.random.rand() < 0.5 else -1)\n",
    "        close = [] # so we can start again with an empty list\n",
    "      else:\n",
    "        # nothing close, let's break\n",
    "        break\n",
    "\n",
    "    placed[i] = (x, y)\n",
    "\n",
    "    plt.annotate(\n",
    "      s=index_word_map[i],\n",
    "      xy=(X[i,0], X[i,1]),\n",
    "      xytext=(x, y),\n",
    "      arrowprops={\n",
    "        'arrowstyle' : '->',\n",
    "        'color' : 'black',\n",
    "      }\n",
    "    )\n",
    "\n",
    "print(\"vocab size:\", current_index)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "#print(X)\n",
    "X = transformer.fit_transform(X).toarray()\n",
    "\n",
    "reducer = TSNE()\n",
    "Z = reducer.fit_transform(X)\n",
    "plot_k_means(Z[:,:2], current_index/10, index_word_map, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
